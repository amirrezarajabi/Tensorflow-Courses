{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca16ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47251afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552895fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361b4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb[\"train\"], imdb[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887e0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences, training_labels = [], []\n",
    "test_sentences, test_labels = [], []\n",
    "\n",
    "for s, l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n",
    "    \n",
    "for s, l in test_data:\n",
    "    test_sentences.append(str(s.numpy()))\n",
    "    test_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46cc893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0e600d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9be972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"This was an absolutely terrible movie. Don\\'t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie\\'s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor\\'s like Christopher Walken\\'s good name. I could barely sit through it.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5bd4ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels_final = np.array(training_labels)\n",
    "test_labels_final = np.array(test_labels)\n",
    "training_labels_final.shape, test_labels_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb100703",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "trunc_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "max_length = 120\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e6c9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, truncating=trunc_type, maxlen=max_length)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "testing_pad = pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c6d0d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <OOV> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <OOV> without any real concern for anything else i cant recommend this film at all '\n",
      "b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(v, k) for (k, v) in word_index.items()])\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, \"?\") for i in text])\n",
    "print(decode_review(padded[1]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2aa023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7f84d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 11526     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8faec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 24s 21ms/step - loss: 0.5108 - acc: 0.7293 - val_loss: 0.3573 - val_acc: 0.8423\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2458 - acc: 0.9072 - val_loss: 0.3686 - val_acc: 0.8385\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.0984 - acc: 0.9747 - val_loss: 0.4504 - val_acc: 0.8250\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0281 - acc: 0.9965 - val_loss: 0.5176 - val_acc: 0.8252\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0107 - acc: 0.9988 - val_loss: 0.5790 - val_acc: 0.8251\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0039 - acc: 0.9996 - val_loss: 0.6381 - val_acc: 0.8238\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.6862 - val_acc: 0.8246\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 5.5362e-04 - acc: 1.0000 - val_loss: 0.7246 - val_acc: 0.8254\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.0601e-04 - acc: 1.0000 - val_loss: 0.7625 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 1.8217e-04 - acc: 1.0000 - val_loss: 0.7980 - val_acc: 0.8255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb2b0b5a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded, training_labels_final, epochs=10, validation_data=(testing_pad, test_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27878ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c29ee604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "out_v = io.open('vecs.tsv', 'w', encoding=\"utf-8\")\n",
    "out_m = io.open('meta.tsv', 'w', encoding=\"utf-8\")\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index.get(word_num)\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word+\"\\n\")\n",
    "    out_v.write(\"\\t\".join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_m.close()\n",
    "out_v.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
